{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentAST(id='my_experiment_id', splitting_fields=['my_fld', 'my_fld_1'], salt='csdvs887', conditions=ExperimentConditional(predicate=RecursivePredicate(left_predicate=RecursivePredicate(left_predicate=TerminalPredicate(left_term=Identifier(name='field1'), logical_operator=<built-in function eq>, right_term='a'), boolean_operator=<built-in function and_>, right_predicate=RecursivePredicate(left_predicate=TerminalPredicate(left_term=Identifier(name='field2'), logical_operator=<built-in function gt>, right_term=4), boolean_operator=<built-in function not_>, right_predicate=None)), boolean_operator=<built-in function or_>, right_predicate=TerminalPredicate(left_term=Identifier(name='field3'), logical_operator=<built-in function lt>, right_term=9)), true_branch=ExperimentConditional(predicate=TerminalPredicate(left_term=Identifier(name='field4'), logical_operator=<built-in function eq>, right_term='xyz'), true_branch=[ExperimentGroup(group_definition=123, group_weight=3.4), ExperimentGroup(group_definition=9, group_weight=5.0), ExperimentGroup(group_definition='abc', group_weight=3.0)], false_branch=[ExperimentGroup(group_definition='Setting1', group_weight=1.0), ExperimentGroup(group_definition='Setting2', group_weight=0.0)]), false_branch=[ExperimentGroup(group_definition='default', group_weight=1.0)]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyab_tester.language.lexer import ExperimentLexer\n",
    "from pyab_tester.language.grammar import ExperimentParser\n",
    "from pyab_tester.data_structures.syntax_tree import ExperimentAST\n",
    "\n",
    "def compile(text:str) -> ExperimentAST:\n",
    "    lexer = ExperimentLexer()\n",
    "    parser = ExperimentParser()\n",
    "    return parser.parse(lexer.tokenize(text))\n",
    "\n",
    "\n",
    "\n",
    "f = open(\"sample_experiments/sample.pyab\", \"r\")\n",
    "ast = compile(f.read()) \n",
    "\n",
    "#f = open(\"sample_experiments/sample_with_tuple.pyab\", \"r\")\n",
    "#ast = compile(f.read())\n",
    "ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(type='DEF', value='def', lineno=3, index=44, end=47),\n",
       " Token(type='ID', value='my_experiment_id', lineno=3, index=48, end=64),\n",
       " Token(type='COLON', value=':', lineno=3, index=64, end=65),\n",
       " Token(type='IF', value='if', lineno=4, index=70, end=72),\n",
       " Token(type='ID', value='field1', lineno=4, index=73, end=79),\n",
       " Token(type='IN', value='in', lineno=4, index=80, end=82),\n",
       " Token(type='LPAREN', value='(', lineno=4, index=83, end=84),\n",
       " Token(type='STRING_LITERAL', value='a', lineno=4, index=84, end=87),\n",
       " Token(type='COMMA', value=',', lineno=4, index=87, end=88),\n",
       " Token(type='STRING_LITERAL', value='b', lineno=4, index=88, end=91),\n",
       " Token(type='COMMA', value=',', lineno=4, index=91, end=92),\n",
       " Token(type='STRING_LITERAL', value='c', lineno=4, index=92, end=95),\n",
       " Token(type='COMMA', value=',', lineno=4, index=95, end=96),\n",
       " Token(type='NON_NEG_INTEGER', value=1, lineno=4, index=96, end=97),\n",
       " Token(type='COMMA', value=',', lineno=4, index=97, end=98),\n",
       " Token(type='NON_NEG_INTEGER', value=2, lineno=4, index=98, end=99),\n",
       " Token(type='COMMA', value=',', lineno=4, index=99, end=100),\n",
       " Token(type='NON_NEG_INTEGER', value=3, lineno=4, index=100, end=101),\n",
       " Token(type='RPAREN', value=')', lineno=4, index=101, end=102),\n",
       " Token(type='AND', value='and', lineno=4, index=103, end=106),\n",
       " Token(type='ID', value='field2', lineno=4, index=107, end=113),\n",
       " Token(type='NOT_IN', value='not in', lineno=4, index=114, end=120),\n",
       " Token(type='LPAREN', value='(', lineno=4, index=121, end=122),\n",
       " Token(type='STRING_LITERAL', value='d', lineno=4, index=122, end=125),\n",
       " Token(type='COMMA', value=',', lineno=4, index=125, end=126),\n",
       " Token(type='STRING_LITERAL', value='e', lineno=4, index=126, end=129),\n",
       " Token(type='COMMA', value=',', lineno=4, index=129, end=130),\n",
       " Token(type='STRING_LITERAL', value='f', lineno=4, index=130, end=133),\n",
       " Token(type='COMMA', value=',', lineno=4, index=133, end=134),\n",
       " Token(type='NON_NEG_INTEGER', value=5, lineno=4, index=134, end=135),\n",
       " Token(type='COMMA', value=',', lineno=4, index=135, end=136),\n",
       " Token(type='NON_NEG_INTEGER', value=6, lineno=4, index=136, end=137),\n",
       " Token(type='COMMA', value=',', lineno=4, index=137, end=138),\n",
       " Token(type='NON_NEG_INTEGER', value=7, lineno=4, index=138, end=139),\n",
       " Token(type='RPAREN', value=')', lineno=4, index=139, end=140),\n",
       " Token(type='LBRACE', value='{', lineno=4, index=140, end=141),\n",
       " Token(type='RETURN', value='return', lineno=5, index=154, end=160),\n",
       " Token(type='STRING_LITERAL', value='Setting1', lineno=5, index=161, end=171),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=5, index=172, end=180),\n",
       " Token(type='NON_NEG_INTEGER', value=1, lineno=5, index=181, end=182),\n",
       " Token(type='COMMA', value=',', lineno=5, index=182, end=183),\n",
       " Token(type='STRING_LITERAL', value='Setting2', lineno=6, index=204, end=214),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=6, index=215, end=223),\n",
       " Token(type='NON_NEG_INTEGER', value=1, lineno=6, index=224, end=225),\n",
       " Token(type='RBRACE', value='}', lineno=8, index=231, end=232)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyab_tester.language.lexer import ExperimentLexer\n",
    "f = open(\"sample_experiments/sample_with_tuple.pyab\", \"r\")\n",
    "lexer = ExperimentLexer()\n",
    "[t for t in lexer.tokenize(f.read())]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc-pli-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
