{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentAST(id='my_experiment_id', splitting_fields=None, salt=None, conditions=ExperimentConditional(predicate=RecursivePredicate(left_predicate=TerminalPredicate(left_term=Identifier(name='field1'), logical_operator=<function operator_in at 0x109034f70>, right_term=('a', 'b', 'c', 1, 2, 3)), boolean_operator=<built-in function and_>, right_predicate=TerminalPredicate(left_term=Identifier(name='field2'), logical_operator=<function operator_not_in at 0x1090dba30>, right_term=('d', 'e', 'f', 5, 6, 7))), true_branch=[ExperimentGroup(group_definition='Setting1', group_weight=1.0), ExperimentGroup(group_definition='Setting2', group_weight=1.0)], false_branch=None))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyab_tester.language.lexer import ExperimentLexer\n",
    "from pyab_tester.language.grammar import ExperimentParser\n",
    "from pyab_tester.data_structures.syntax_tree import ExperimentAST\n",
    "\n",
    "def compile(text:str) -> ExperimentAST:\n",
    "    lexer = ExperimentLexer()\n",
    "    parser = ExperimentParser()\n",
    "    return parser.parse(lexer.tokenize(text))\n",
    "\n",
    "\n",
    "\n",
    "f = open(\"sample_experiments/sample.pyab\", \"r\")\n",
    "ast = compile(f.read())\n",
    "\n",
    "f = open(\"sample_experiments/sample_with_tuple.pyab\", \"r\")\n",
    "ast = compile(f.read())\n",
    "ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(type='DEF', value='def', lineno=7, index=253, end=256),\n",
       " Token(type='ID', value='my_experiment_id', lineno=7, index=257, end=273),\n",
       " Token(type='LBRACE', value='{', lineno=7, index=273, end=274),\n",
       " Token(type='SALT', value='salt', lineno=9, index=339, end=343),\n",
       " Token(type='COLON', value=':', lineno=9, index=343, end=344),\n",
       " Token(type='STRING_LITERAL', value='csdvs887', lineno=9, index=345, end=355),\n",
       " Token(type='SPLITTERS', value='splitters', lineno=12, index=433, end=442),\n",
       " Token(type='COLON', value=':', lineno=12, index=442, end=443),\n",
       " Token(type='ID', value='my_fld', lineno=12, index=444, end=450),\n",
       " Token(type='COMMA', value=',', lineno=12, index=450, end=451),\n",
       " Token(type='ID', value='my_fld_1', lineno=12, index=452, end=460),\n",
       " Token(type='IF', value='if', lineno=20, index=758, end=760),\n",
       " Token(type='ID', value='field1', lineno=20, index=761, end=767),\n",
       " Token(type='EQ', value='==', lineno=20, index=767, end=769),\n",
       " Token(type='STRING_LITERAL', value='a', lineno=20, index=769, end=772),\n",
       " Token(type='AND', value='and', lineno=20, index=773, end=776),\n",
       " Token(type='NOT', value='not', lineno=20, index=777, end=780),\n",
       " Token(type='ID', value='field2', lineno=20, index=781, end=787),\n",
       " Token(type='GT', value='>', lineno=20, index=788, end=789),\n",
       " Token(type='NON_NEG_INTEGER', value=4, lineno=20, index=789, end=790),\n",
       " Token(type='OR', value='or', lineno=20, index=791, end=793),\n",
       " Token(type='ID', value='field3', lineno=20, index=794, end=800),\n",
       " Token(type='LT', value='<', lineno=20, index=800, end=801),\n",
       " Token(type='NON_NEG_INTEGER', value=9, lineno=20, index=801, end=802),\n",
       " Token(type='LBRACE', value='{', lineno=20, index=802, end=803),\n",
       " Token(type='IF', value='if', lineno=21, index=812, end=814),\n",
       " Token(type='ID', value='field4', lineno=21, index=815, end=821),\n",
       " Token(type='EQ', value='==', lineno=21, index=822, end=824),\n",
       " Token(type='STRING_LITERAL', value='xyz', lineno=21, index=825, end=830),\n",
       " Token(type='LBRACE', value='{', lineno=21, index=830, end=831),\n",
       " Token(type='RETURN', value='return', lineno=25, index=995, end=1001),\n",
       " Token(type='NON_NEG_INTEGER', value=123, lineno=25, index=1002, end=1005),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=25, index=1006, end=1014),\n",
       " Token(type='NON_NEG_FLOAT', value=3.4, lineno=25, index=1015, end=1018),\n",
       " Token(type='COMMA', value=',', lineno=25, index=1018, end=1019),\n",
       " Token(type='NON_NEG_FLOAT', value=9.3, lineno=26, index=1040, end=1043),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=26, index=1044, end=1052),\n",
       " Token(type='NON_NEG_INTEGER', value=5, lineno=26, index=1053, end=1054),\n",
       " Token(type='COMMA', value=',', lineno=26, index=1054, end=1055),\n",
       " Token(type='STRING_LITERAL', value='abc', lineno=27, index=1076, end=1081),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=27, index=1082, end=1090),\n",
       " Token(type='NON_NEG_INTEGER', value=3, lineno=27, index=1091, end=1092),\n",
       " Token(type='RBRACE', value='}', lineno=28, index=1101, end=1102),\n",
       " Token(type='ELIF', value='else if', lineno=29, index=1111, end=1118),\n",
       " Token(type='ID', value='field5', lineno=29, index=1119, end=1125),\n",
       " Token(type='NE', value='!=', lineno=29, index=1126, end=1128),\n",
       " Token(type='STRING_LITERAL', value='x', lineno=29, index=1129, end=1132),\n",
       " Token(type='LBRACE', value='{', lineno=29, index=1132, end=1133),\n",
       " Token(type='RETURN', value='return', lineno=30, index=1146, end=1152),\n",
       " Token(type='STRING_LITERAL', value='Setting 1.1', lineno=30, index=1153, end=1166),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=30, index=1167, end=1175),\n",
       " Token(type='NON_NEG_INTEGER', value=1, lineno=30, index=1176, end=1177),\n",
       " Token(type='COMMA', value=',', lineno=30, index=1177, end=1178),\n",
       " Token(type='STRING_LITERAL', value='Setting 1.2', lineno=31, index=1199, end=1212),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=31, index=1213, end=1221),\n",
       " Token(type='NON_NEG_INTEGER', value=0, lineno=31, index=1222, end=1223),\n",
       " Token(type='RBRACE', value='}', lineno=33, index=1233, end=1234),\n",
       " Token(type='ELSE', value='else', lineno=34, index=1243, end=1247),\n",
       " Token(type='LBRACE', value='{', lineno=34, index=1247, end=1248),\n",
       " Token(type='RETURN', value='return', lineno=35, index=1261, end=1267),\n",
       " Token(type='STRING_LITERAL', value='Setting1', lineno=35, index=1268, end=1278),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=35, index=1279, end=1287),\n",
       " Token(type='NON_NEG_INTEGER', value=1, lineno=35, index=1288, end=1289),\n",
       " Token(type='COMMA', value=',', lineno=35, index=1289, end=1290),\n",
       " Token(type='STRING_LITERAL', value='Setting2', lineno=36, index=1311, end=1321),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=36, index=1322, end=1330),\n",
       " Token(type='NON_NEG_INTEGER', value=0, lineno=36, index=1331, end=1332),\n",
       " Token(type='RBRACE', value='}', lineno=38, index=1342, end=1343),\n",
       " Token(type='RBRACE', value='}', lineno=39, index=1348, end=1349),\n",
       " Token(type='ELSE', value='else', lineno=40, index=1354, end=1358),\n",
       " Token(type='LBRACE', value='{', lineno=40, index=1358, end=1359),\n",
       " Token(type='RETURN', value='return', lineno=41, index=1368, end=1374),\n",
       " Token(type='STRING_LITERAL', value='default', lineno=41, index=1375, end=1384),\n",
       " Token(type='WEIGHTED', value='weighted', lineno=41, index=1385, end=1393),\n",
       " Token(type='NON_NEG_INTEGER', value=1, lineno=41, index=1394, end=1395),\n",
       " Token(type='RBRACE', value='}', lineno=42, index=1447, end=1448),\n",
       " Token(type='RBRACE', value='}', lineno=43, index=1449, end=1450)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyab_tester.language.lexer import ExperimentLexer\n",
    "f = open(\"sample_experiments/sample.pyab\", \"r\")\n",
    "lexer = ExperimentLexer()\n",
    "[t for t in lexer.tokenize(f.read())]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc-pli-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
